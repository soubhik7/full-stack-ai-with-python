{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d48ae55",
   "metadata": {},
   "source": [
    "\n",
    "# MNIST Binary Classification (5 vs Not 5)\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. Loading MNIST dataset\n",
    "2. Converting to binary classification (Digit 5 vs Not 5)\n",
    "3. Training a classifier\n",
    "4. Evaluating using:\n",
    "   - Cross Validation\n",
    "   - Confusion Matrix\n",
    "   - Precision, Recall, F1 Score\n",
    "   - Precision–Recall Tradeoff\n",
    "   - ROC Curve\n",
    "   - ROC AUC Score\n",
    "\n",
    "This notebook is beginner-friendly and well-commented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5f2da",
   "metadata": {},
   "source": [
    "\n",
    "# Detailed Explanation of Evaluation Metrics\n",
    "\n",
    "## 1️⃣ Cross Validation\n",
    "\n",
    "Cross-validation is a robust evaluation technique where the dataset is split into multiple folds (here 5 folds).\n",
    "The model is trained on 4 folds and tested on the remaining fold. This process repeats 5 times.\n",
    "\n",
    "Why use it?\n",
    "- Reduces overfitting risk\n",
    "- Provides more reliable accuracy\n",
    "- Uses entire dataset for training and validation\n",
    "\n",
    "Key Output:\n",
    "- Mean Accuracy → Overall performance\n",
    "- Standard Deviation → Model stability\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Confusion Matrix\n",
    "\n",
    "A confusion matrix shows how predictions compare with actual values.\n",
    "\n",
    "Structure (Binary Classification):\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|-------------------|-------------------|\n",
    "| Actual Negative| True Negative (TN)| False Positive (FP)|\n",
    "| Actual Positive| False Negative (FN)| True Positive (TP)|\n",
    "\n",
    "Interpretation:\n",
    "- TN → Correctly predicted non-5 digits\n",
    "- TP → Correctly predicted digit 5\n",
    "- FP → Incorrectly predicted 5\n",
    "- FN → Missed actual 5\n",
    "\n",
    "This helps understand the type of errors your model makes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Precision, Recall, and F1 Score\n",
    "\n",
    "### Precision\n",
    "Out of all predicted positives, how many were actually correct?\n",
    "Formula:\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "High precision → Few false positives.\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "Out of all actual positives, how many were detected?\n",
    "Formula:\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "High recall → Few missed positives.\n",
    "\n",
    "### F1 Score\n",
    "Harmonic mean of Precision and Recall.\n",
    "Formula:\n",
    "F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "Useful when dataset is imbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Precision–Recall Tradeoff\n",
    "\n",
    "Models output scores (confidence values). By changing the classification threshold:\n",
    "- Increasing threshold → Precision increases, Recall decreases\n",
    "- Decreasing threshold → Recall increases, Precision decreases\n",
    "\n",
    "This curve helps choose the best threshold depending on business need.\n",
    "\n",
    "Example:\n",
    "- Medical diagnosis → High Recall preferred\n",
    "- Spam detection → High Precision preferred\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "ROC curve plots:\n",
    "\n",
    "- X-axis → False Positive Rate (FPR)\n",
    "- Y-axis → True Positive Rate (Recall)\n",
    "\n",
    "It shows model performance across all thresholds.\n",
    "\n",
    "The diagonal dashed line represents a random classifier.\n",
    "\n",
    "Better models curve toward the top-left corner.\n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ ROC AUC Score\n",
    "\n",
    "AUC = Area Under the ROC Curve.\n",
    "\n",
    "Interpretation:\n",
    "- 1.0 → Perfect classifier\n",
    "- 0.9–0.99 → Excellent\n",
    "- 0.8–0.89 → Good\n",
    "- 0.7–0.79 → Fair\n",
    "- 0.5 → Random guessing\n",
    "\n",
    "Higher AUC means better separability between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faf0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7666573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"].astype(np.int8)\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Label shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to binary classification (5 vs Not 5)\n",
    "y_binary = (y == 5)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y_binary[:60000], y_binary[60000:]\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model\n",
    "model = SGDClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321bf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross Validation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(\"Cross Validation Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation:\", cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix using cross-validation predictions\n",
    "y_train_pred = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precision, Recall, F1 Score\n",
    "precision = precision_score(y_train, y_train_pred)\n",
    "recall = recall_score(y_train, y_train_pred)\n",
    "f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b46b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision scores for threshold analysis\n",
    "y_scores = cross_val_predict(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    method=\"decision_function\"\n",
    ")\n",
    "\n",
    "print(\"Decision scores shape:\", y_scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4155e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precision-Recall Curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, precisions[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall vs Threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8930f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_train, y_scores)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7209e",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- Cross Validation provides stable accuracy.\n",
    "- Confusion Matrix shows classification errors.\n",
    "- Precision and Recall measure positive prediction performance.\n",
    "- Precision–Recall tradeoff helps choose threshold.\n",
    "- ROC Curve evaluates performance across thresholds.\n",
    "- ROC AUC summarizes separability.\n",
    "\n",
    "This completes the full evaluation pipeline for binary classification.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
